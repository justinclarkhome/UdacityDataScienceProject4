{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras import utils as np_utils\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt       \n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image         \n",
    "from tqdm import tqdm\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from PIL import ImageFile\n",
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "random.seed(8675309)\n",
    "      \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Flow\n",
    "\n",
    "* 0: Import Datasets\n",
    "* 1: Detect Humans\n",
    "* 2: Detect Dogs\n",
    "* 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* 4: Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* 6: Write your Algorithm\n",
    "* 7: Test Your Algorithm\n",
    "* 8: Build/deploy webapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_human_files(data_dir='./lfw'):\n",
    "    # load filenames in shuffled human dataset\n",
    "    human_files = np.array(glob(f'{data_dir}/*/*'))\n",
    "    random.shuffle(human_files)\n",
    "\n",
    "    # print statistics about the dataset\n",
    "    print('There are %d total human images.' % len(human_files))\n",
    "    return human_files\n",
    "\n",
    "def load_dog_data(\n",
    "        data_dir='./dogImages',\n",
    "        ):\n",
    "    \"\"\" Wrapper for function that loads the dog dataset files.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str, optional): Directory to load from. Defaults to './dogImages'.\n",
    "    \"\"\"\n",
    "    def load_dataset(path):\n",
    "        data = load_files(path)\n",
    "        dog_files = np.array(data['filenames'])\n",
    "        dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "        return dog_files, dog_targets\n",
    "\n",
    "    # load train, test, and validation datasets\n",
    "    train_files, train_targets = load_dataset('dogImages/train')\n",
    "    valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "    test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "    # load list of dog names\n",
    "    dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "    return (\n",
    "        train_files, train_targets, \n",
    "        valid_files, valid_targets, \n",
    "        test_files, test_targets, \n",
    "        dog_names\n",
    "    )\n",
    "\n",
    "def load_open_cv_face_data(human_files, plot=True):\n",
    "    # extract pre-trained face detector\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "    # load color (BGR) image\n",
    "    img = cv2.imread(human_files[3])\n",
    "    # convert BGR image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # find faces in image\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "    # print number of faces detected in the image\n",
    "    print('Number of faces detected:', len(faces))\n",
    "\n",
    "    # get bounding box for each detected face\n",
    "    for (x,y,w,h) in faces:\n",
    "        # add bounding box to color image\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        \n",
    "    # convert BGR image to RGB for plotting\n",
    "    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if plot:\n",
    "        # display the image, along with bounding box\n",
    "        plt.imshow(cv_rgb)\n",
    "        plt.show()\n",
    "\n",
    "    return face_cascade, faces\n",
    "\n",
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path, face_cascade):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_files, train_targets, \n",
    "        valid_files, valid_targets, \n",
    "        test_files, test_targets, \n",
    "        dog_names\n",
    ") = load_dog_data()\n",
    "\n",
    "human_files = load_human_files()\n",
    "face_cascade, faces = load_open_cv_face_data(human_files, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Question 1:__\n",
    "Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ \n",
    "\n",
    "The face detector identified 100% of humans as humans but it also identfiied 12% of the dogs as human too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for label, files in {\n",
    "    'humans as humans': human_files[:100],\n",
    "    'dogs as humans': train_files[:100],\n",
    "    }.items():\n",
    "    detected_count = np.array([face_detector(i, face_cascade) for i in files]).sum()\n",
    "    detected_pct = detected_count/len(files)\n",
    "    print(f'Deteced {detected_pct:.1%} of {len(files)} {label}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thank you:\n",
    "# https://www.analyticsvidhya.com/blog/2019/01/build-image-classification-model-10-minutes/?utm_source=blog&utm_source=learn-image-classification-cnn-convolutional-neural-networks-5-datasets\n",
    "# https://www.analyticsvidhya.com/blog/2018/12/guide-convolutional-neural-network-cnn/\n",
    "\n",
    "class JustinDogCNN(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "            self, \n",
    "            loss_function='sparse_categorical_crossentropy', \n",
    "            optimizer='adam',\n",
    "            eval_metrics=['accuracy']\n",
    "            ):\n",
    "        self.model = None\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.eval_metrics = eval_metrics\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        input_shape = X.shape[1:]\n",
    "        self.model = models.Sequential()\n",
    "        self.model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "        self.model.add(layers.MaxPooling2D((2, 2)))\n",
    "        self.model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        self.model.add(layers.MaxPooling2D((2, 2)))\n",
    "        self.model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        self.model.add(layers.Flatten())\n",
    "        self.model.add(layers.Dense(64, activation='relu'))\n",
    "        self.model.add(layers.Dense(10, activation='softmax'))  # Assuming 10 classes\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=self.optimizer,\n",
    "            loss=self.loss_function,\n",
    "            metrics=self.eval_metrics,\n",
    "            )\n",
    "\n",
    "        self.model.fit(X, y, epochs=10, batch_size=32, verbose=1)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "pipeline = Pipeline([\n",
    "    ('cnn', JustinDogCNN())\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "# test_loss, test_acc = pipeline.named_steps['cnn'].model.evaluate(X_test, y_test, verbose=2)\n",
    "# print(f\"Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
